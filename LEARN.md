# üß† Learn Neural Networks: A Complete Guide

Welcome to this comprehensive guide on understanding the Multi-Layer Perceptron (MLP) neural network implemented in this project. This document will take you from the fundamentals to advanced concepts, using the actual code as examples.

---

## üìö Table of Contents

1. [What is a Neural Network?](#what-is-a-neural-network)
2. [The Neuron: Building Block of Neural Networks](#the-neuron-building-block-of-neural-networks)
3. [Network Architecture](#network-architecture)
4. [Forward Propagation](#forward-propagation)
5. [Activation Functions](#activation-functions)
6. [Loss Function (Cost)](#loss-function-cost)
7. [Backpropagation](#backpropagation)
8. [Gradient Descent](#gradient-descent)
9. [Training Process](#training-process)
10. [Data Augmentation](#data-augmentation)
11. [Practical Tips](#practical-tips)
12. [Mathematical Foundations](#mathematical-foundations)
13. [Glossary](#glossary)

---

## What is a Neural Network?

A **neural network** is a computational model inspired by the human brain. Just as our brain uses interconnected neurons to process information, artificial neural networks use mathematical "neurons" to learn patterns from data.

### Real-World Analogy

Imagine teaching a child to recognize handwritten digits:
1. You show them many examples of "3"
2. They start noticing patterns (two curves, specific shape)
3. Eventually, they can recognize "3" even in different handwriting styles

Neural networks learn the same way‚Äîthrough **examples and feedback**.

### Why "Multi-Layer"?

A Multi-Layer Perceptron has multiple layers of neurons:

```
Input ‚Üí [Hidden Layer 1] ‚Üí [Hidden Layer 2] ‚Üí Output
```

Each layer learns increasingly abstract features:
- **Layer 1**: Detects edges, curves, basic shapes
- **Layer 2**: Combines these into more complex patterns
- **Output**: Makes the final decision (which digit 0-9?)

---

## The Neuron: Building Block of Neural Networks

### What Does a Neuron Do?

A single neuron performs three steps:

1. **Receive inputs** (weighted connections from previous layer)
2. **Sum them up** (add all weighted inputs + bias)
3. **Apply activation** (decide how strongly to "fire")

### Mathematical Formula

$$z = \sum_{i=1}^{n} (w_i \cdot x_i) + b$$

$$a = \sigma(z)$$

Where:
- $x_i$ = input values
- $w_i$ = weights (importance of each input)
- $b$ = bias (threshold adjustment)
- $z$ = weighted sum
- $\sigma$ = activation function
- $a$ = output (activation)

### In Our Code

```javascript
// From script.js - Forward propagation through a layer
for (let n = 0; n < layer.neurons.length; n++) {
    let neuron = layer.neurons[n];
    let sum = neuron.b;  // Start with bias
    
    // Add weighted inputs
    for (let w = 0; w < neuron.w.length; w++) {
        sum += neuron.w[w] * inputValues[w];
    }
    
    // Apply activation function
    neuron.z = sum;
    neuron.a = layer.activationFunction(sum);  // TanH or Softmax
}
```

---

## Network Architecture

Our network has a specific architecture designed for digit recognition:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NETWORK ARCHITECTURE                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  INPUT LAYER          HIDDEN LAYERS           OUTPUT LAYER      ‚îÇ
‚îÇ  (400 neurons)        (16 + 16 neurons)       (10 neurons)      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ 1 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 1 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 1 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 0 ‚îÇ P(0)       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚î§    Weights     ‚îú‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚î§          ‚îú‚îÄ‚îÄ‚îÄ‚î§            ‚îÇ
‚îÇ  ‚îÇ 2 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 2 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 2 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 1 ‚îÇ P(1)       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚î§                ‚îú‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚î§          ‚îú‚îÄ‚îÄ‚îÄ‚î§            ‚îÇ
‚îÇ  ‚îÇ...‚îÇ                ‚îÇ...‚îÇ    ‚îÇ...‚îÇ          ‚îÇ...‚îÇ            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚î§                ‚îú‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚î§          ‚îú‚îÄ‚îÄ‚îÄ‚î§            ‚îÇ
‚îÇ  ‚îÇ400‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ16 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ16 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 9 ‚îÇ P(9)       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  20√ó20 pixel          TanH      TanH          Softmax          ‚îÇ
‚îÇ  image grid         activation activation    (probabilities)   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Layer Breakdown

| Layer | Neurons | Purpose | Activation |
|-------|---------|---------|------------|
| Input | 400 | Raw pixel values (20√ó20 grid) | None |
| Hidden 1 | 16 | Learn basic features (edges, curves) | TanH |
| Hidden 2 | 16 | Combine features into patterns | TanH |
| Output | 10 | Probability for each digit (0-9) | Softmax |

### Why These Numbers?

- **400 inputs**: 20√ó20 = 400 pixels in our drawing grid
- **16 hidden neurons**: Balance between capacity and training speed
- **10 outputs**: One for each digit (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)

---

## Forward Propagation

**Forward propagation** is the process of passing an input through the network to get a prediction.

### Step-by-Step Process

```
Step 1: Normalize input pixels (0-1 range)
           ‚Üì
Step 2: Feed to Hidden Layer 1 (400 ‚Üí 16)
           ‚Üì
Step 3: Apply TanH activation
           ‚Üì
Step 4: Feed to Hidden Layer 2 (16 ‚Üí 16)
           ‚Üì
Step 5: Apply TanH activation
           ‚Üì
Step 6: Feed to Output Layer (16 ‚Üí 10)
           ‚Üì
Step 7: Apply Softmax (get probabilities)
           ‚Üì
Step 8: Return highest probability digit
```

### In Our Code

```javascript
function forwardprop(image) {
    let inputValues = image;  // 400 pixel values
    
    // Process each layer
    for (let l = 0; l < nn.layers.length; l++) {
        let layer = nn.layers[l];
        let outputValues = [];
        
        // Process each neuron in this layer
        for (let n = 0; n < layer.neurons.length; n++) {
            let neuron = layer.neurons[n];
            let sum = neuron.b;
            
            for (let w = 0; w < neuron.w.length; w++) {
                sum += neuron.w[w] * inputValues[w];
            }
            
            neuron.z = sum;
            outputValues.push(sum);
        }
        
        // Apply activation function to entire layer
        if (layer.activation === "tanh") {
            outputValues = outputValues.map(z => TanH(z));
        } else if (layer.activation === "softmax") {
            outputValues = softmax(outputValues);
        }
        
        inputValues = outputValues;  // Output becomes input for next layer
    }
    
    return inputValues;  // Final output: 10 probabilities
}
```

---

## Activation Functions

Activation functions introduce **non-linearity** into the network. Without them, stacking layers would be pointless‚Äîmultiple linear transformations can be collapsed into a single transformation.

### TanH (Hyperbolic Tangent)

Used in our hidden layers.

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**Properties:**
- Output range: **-1 to +1**
- Zero-centered (helps training)
- Smooth gradient

```javascript
// From maths.js
function TanH(x) {
    return (Math.exp(x) - Math.exp(-x)) / (Math.exp(x) + Math.exp(-x));
}

// Derivative (used in backprop)
function dTanH(x) {
    return 1 - Math.pow(TanH(x), 2);
}
```

**Visual:**
```
    1 |          ___________
      |         /
    0 |--------/------------
      |       /
   -1 |______/
      -4  -2   0   2   4
```

### Softmax

Used in our output layer to convert raw scores to probabilities.

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}$$

**Properties:**
- All outputs sum to 1 (valid probability distribution)
- Emphasizes the highest value
- Perfect for classification

```javascript
// From maths.js
function softmax(arr) {
    const maxVal = Math.max(...arr);
    const expValues = arr.map(x => Math.exp(x - maxVal));  // Numerical stability
    const sumExp = expValues.reduce((a, b) => a + b, 0);
    return expValues.map(x => x / sumExp);
}
```

**Example:**
```
Raw scores:    [2.0, 1.0, 0.1]
After softmax: [0.659, 0.242, 0.099]  ‚Üê Probabilities sum to 1.0
```

---

## Loss Function (Cost)

The **loss function** measures how wrong our predictions are. We want to minimize this!

### Cross-Entropy Loss

We use cross-entropy loss, perfect for classification:

$$L = -\sum_{i=1}^{n} y_i \cdot \log(\hat{y}_i)$$

Where:
- $y_i$ = true label (1 for correct class, 0 otherwise)
- $\hat{y}_i$ = predicted probability

### Why Cross-Entropy?

- Heavily penalizes confident wrong predictions
- Works perfectly with Softmax output
- Produces smooth gradients for learning

```javascript
// From script.js - Calculate loss for one sample
function cost(targetdigit) {
    let sum = 0;
    let outputs = nn.layers[nn.layers.length - 1].neurons;
    
    for (let i = 0; i < outputs.length; i++) {
        let target = (i === targetdigit) ? 1 : 0;
        let predicted = Math.max(outputs[i].a, 1e-15);  // Avoid log(0)
        sum += target * Math.log(predicted);
    }
    
    return -sum;
}
```

### Loss Interpretation

| Loss Value | Meaning |
|------------|---------|
| 0.0 - 0.1 | Excellent predictions |
| 0.1 - 0.5 | Good predictions |
| 0.5 - 1.0 | Moderate predictions |
| 1.0 - 2.0 | Poor predictions |
| > 2.0 | Very poor (random guessing) |

---

## Backpropagation

**Backpropagation** is how the network learns from mistakes. It calculates how much each weight contributed to the error.

### The Chain Rule

Backprop uses calculus's chain rule to compute gradients:

$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}$$

### Step-by-Step

```
1. Calculate output error (prediction vs target)
          ‚Üì
2. Compute gradients for output layer weights
          ‚Üì
3. Propagate error backward through hidden layers
          ‚Üì
4. Compute gradients for each hidden layer
          ‚Üì
5. Accumulate gradients for all samples in batch
          ‚Üì
6. Update weights using gradient descent
```

### In Our Code

```javascript
function backprop(targetdigit) {
    // STEP 1: Output layer gradients
    let outputLayer = nn.layers[nn.layers.length - 1];
    for (let n = 0; n < outputLayer.neurons.length; n++) {
        let neuron = outputLayer.neurons[n];
        let target = (n === targetdigit) ? 1 : 0;
        
        // Derivative of cross-entropy + softmax combined
        neuron.delta = neuron.a - target;
    }
    
    // STEP 2: Hidden layer gradients (propagate backward)
    for (let l = nn.layers.length - 2; l >= 0; l--) {
        let layer = nn.layers[l];
        let nextLayer = nn.layers[l + 1];
        
        for (let n = 0; n < layer.neurons.length; n++) {
            let neuron = layer.neurons[n];
            let errorSum = 0;
            
            // Sum error from all neurons this connects to
            for (let nn = 0; nn < nextLayer.neurons.length; nn++) {
                errorSum += nextLayer.neurons[nn].delta * 
                           nextLayer.neurons[nn].w[n];
            }
            
            // Multiply by activation derivative
            neuron.delta = errorSum * dTanH(neuron.z);
        }
    }
    
    // STEP 3: Accumulate weight gradients
    accumulateGradients();
}
```

### Visual Explanation

```
Forward:  Input ‚îÄ‚îÄ‚ñ∂ Hidden ‚îÄ‚îÄ‚ñ∂ Output ‚îÄ‚îÄ‚ñ∂ Loss
                                           ‚îÇ
Backward: ‚àÇw ‚óÄ‚îÄ‚îÄ ‚àÇhidden ‚óÄ‚îÄ‚îÄ ‚àÇoutput ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         (gradients flow backward)
```

---

## Gradient Descent

**Gradient descent** is the optimization algorithm that updates weights to minimize loss.

### The Update Rule

$$w_{new} = w_{old} - \eta \cdot \frac{\partial L}{\partial w}$$

Where:
- $\eta$ = learning rate (how big a step to take)
- $\frac{\partial L}{\partial w}$ = gradient (direction of steepest increase)

### In Our Code

```javascript
function updateweights() {
    for (let l = 0; l < nn.layers.length; l++) {
        for (let n = 0; n < nn.layers[l].neurons.length; n++) {
            let neuron = nn.layers[l].neurons[n];
            
            // Update each weight
            for (let w = 0; w < neuron.w.length; w++) {
                neuron.w[w] -= learningrate * neuron.gw[w];
                neuron.gw[w] = 0;  // Reset gradient accumulator
            }
            
            // Update bias
            neuron.b -= learningrate * neuron.gb;
            neuron.gb = 0;
        }
    }
}
```

### Learning Rate Effects

| Learning Rate | Effect |
|---------------|--------|
| Too high (0.1+) | Overshoots minimum, unstable training |
| Too low (0.0001) | Very slow convergence |
| Just right (0.001-0.01) | Steady progress toward minimum |

### Visual: Gradient Descent

```
Cost
  ‚îÇ  ‚ï≤
  ‚îÇ   ‚ï≤       Start here ‚óè
  ‚îÇ    ‚ï≤                  ‚ï≤
  ‚îÇ     ‚ï≤                  ‚ï≤
  ‚îÇ      ‚ï≤    Step 1 ‚óè      ‚ï≤
  ‚îÇ       ‚ï≤            ‚ï≤     ‚ï≤
  ‚îÇ        ‚ï≤  Step 2 ‚óè  ‚ï≤     ‚ï≤
  ‚îÇ         ‚ï≤         ‚ï≤  ‚ï≤     ‚ï≤
  ‚îÇ          ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè     ‚ï≤
  ‚îÇ          Minimum!             ‚ï≤
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Weights
```

---

## Training Process

### The Training Loop

```javascript
function train_button() {
    for (let sample = 0; sample < samplenumber; sample++) {
        // 1. Get a random training sample
        let digit = Math.floor(Math.random() * 10);
        let image = getTrainingImage(digit);
        
        // 2. Optionally augment the data
        if (useDataAugmentation) {
            image = randomnessadder(image);
        }
        
        // 3. Forward pass (get prediction)
        forwardprop(image);
        
        // 4. Calculate loss
        let loss = cost(digit);
        
        // 5. Backward pass (compute gradients)
        backprop(digit);
    }
    
    // 6. Update weights based on accumulated gradients
    updateweights();
    
    // 7. Test accuracy
    let accuracy = testAccuracy();
}
```

### Training Tips

1. **Start with small learning rate** (0.01)
2. **Use enough samples per iteration** (50-100)
3. **Enable data augmentation** for better generalization
4. **Watch the loss curve** - should decrease over time
5. **Monitor accuracy** - should increase over time

### What to Look For

| Behavior | Meaning | Action |
|----------|---------|--------|
| Loss decreasing smoothly | Good training | Continue |
| Loss oscillating wildly | Learning rate too high | Reduce learning rate |
| Loss not decreasing | Learning rate too low or stuck | Increase LR or re-init |
| Accuracy plateaus | Model converged | Stop training |

---

## Data Augmentation

Data augmentation artificially expands the training set by applying transformations. This helps the network generalize better.

### Why Augment?

```
Without augmentation:
- Network memorizes exact pixel positions
- Fails on slightly different handwriting
- Overfits to training data

With augmentation:
- Network learns robust features
- Handles variations in handwriting
- Generalizes to new examples
```

### Our Augmentation Functions

#### 1. Rotation (¬±15¬∞)

```javascript
function rotateimage(image, angle) {
    const radians = (angle * Math.PI) / 180;
    const cos = Math.cos(radians);
    const sin = Math.sin(radians);
    
    for (let y = 0; y < 20; y++) {
        for (let x = 0; x < 20; x++) {
            // Rotate around center
            let nx = cos * (x - 10) - sin * (y - 10) + 10;
            let ny = sin * (x - 10) + cos * (y - 10) + 10;
            
            // Sample from rotated position
            newImage[y * 20 + x] = bilinearSample(image, nx, ny);
        }
    }
}
```

#### 2. Translation (¬±4 pixels)

```javascript
function translateimage(image, dx, dy) {
    for (let y = 0; y < 20; y++) {
        for (let x = 0; x < 20; x++) {
            let srcX = x - dx;
            let srcY = y - dy;
            
            if (srcX >= 0 && srcX < 20 && srcY >= 0 && srcY < 20) {
                newImage[y * 20 + x] = image[srcY * 20 + srcX];
            }
        }
    }
}
```

#### 3. Noise (Random pixel variations)

```javascript
function addNoise(image, intensity) {
    return image.map(pixel => {
        let noise = (Math.random() - 0.5) * intensity;
        return Math.max(0, Math.min(1, pixel + noise));
    });
}
```

### Visual Example

```
Original:     Rotated:      Translated:   Noisy:
  ‚ñà‚ñà‚ñà           ‚ñà‚ñà‚ñå            ‚ñà‚ñà‚ñà        ‚ñë‚ñà‚ñà‚ñë
 ‚ñà   ‚ñà        ‚ñà‚ñà  ‚ñà           ‚ñà   ‚ñà       ‚ñà ‚ñë ‚ñà
   ‚ñà‚ñà           ‚ñà‚ñà              ‚ñà‚ñà        ‚ñë ‚ñà‚ñà‚ñë
 ‚ñà   ‚ñà        ‚ñà   ‚ñà           ‚ñà   ‚ñà       ‚ñà‚ñë  ‚ñà
  ‚ñà‚ñà‚ñà          ‚ñà‚ñà‚ñà             ‚ñà‚ñà‚ñà        ‚ñë‚ñà‚ñà‚ñí‚ñë
```

---

## Practical Tips

### Getting Good Results

1. **Draw clearly** - Fill the canvas well
2. **Center your digit** - The network expects centered input
3. **Train enough** - At least 50-100 iterations
4. **Use augmentation** - Improves generalization

### Debugging Training

| Problem | Possible Cause | Solution |
|---------|----------------|----------|
| Accuracy stuck at 10% | Random guessing | Check data loading |
| Loss is NaN | Numerical overflow | Reduce learning rate |
| Training too slow | Many samples | Reduce sample count |
| Poor accuracy on test | Overfitting | Enable augmentation |

### Hyperparameter Tuning

```javascript
// Recommended starting values
let learningrate = 0.01;      // Try: 0.001 to 0.1
let samplenumber = 50;        // Try: 20 to 200
let iterations = 100;         // Try: 50 to 500
let useDataAugmentation = true;
```

---

## Mathematical Foundations

### Matrix Form of Forward Propagation

For a layer, the computation can be written as:

$$\mathbf{z} = \mathbf{W} \cdot \mathbf{x} + \mathbf{b}$$
$$\mathbf{a} = \sigma(\mathbf{z})$$

Where:
- $\mathbf{x}$ is the input vector
- $\mathbf{W}$ is the weight matrix
- $\mathbf{b}$ is the bias vector
- $\mathbf{z}$ is the pre-activation
- $\mathbf{a}$ is the activation output

### Weight Initialization

We use Xavier initialization for better training:

$$w \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in} + n_{out}}}\right)$$

```javascript
// From script.js
function initializeWeights(numInputs, numOutputs) {
    const stdDev = Math.sqrt(2 / (numInputs + numOutputs));
    return Array(numInputs).fill(0).map(() => 
        gaussianRandom() * stdDev
    );
}
```

### Gradient Formulas

**Output layer (with softmax + cross-entropy):**
$$\delta_o = \hat{y} - y$$

**Hidden layer:**
$$\delta_h = (\mathbf{W}_{next}^T \cdot \delta_{next}) \odot \sigma'(\mathbf{z}_h)$$

**Weight gradient:**
$$\frac{\partial L}{\partial w_{ij}} = a_j \cdot \delta_i$$

**Bias gradient:**
$$\frac{\partial L}{\partial b_i} = \delta_i$$

---

## Glossary

| Term | Definition |
|------|------------|
| **Activation** | Output of a neuron after applying activation function |
| **Backpropagation** | Algorithm to compute gradients by propagating error backward |
| **Batch** | Set of samples processed before updating weights |
| **Bias** | Learnable offset added to weighted sum |
| **Cross-Entropy** | Loss function for classification problems |
| **Epoch** | One complete pass through all training data |
| **Forward Propagation** | Computing output by passing input through layers |
| **Gradient** | Derivative indicating direction of steepest increase |
| **Gradient Descent** | Optimization algorithm that follows negative gradient |
| **Hidden Layer** | Layer between input and output |
| **Learning Rate** | Step size for weight updates |
| **Loss/Cost** | Measure of prediction error |
| **Neuron** | Basic computational unit that computes weighted sum + activation |
| **Overfitting** | Model memorizes training data, poor generalization |
| **Softmax** | Activation that converts scores to probabilities |
| **TanH** | Activation function with output range [-1, 1] |
| **Weight** | Learnable parameter controlling connection strength |

---

## Further Reading

- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - Excellent visual explanations
- [Michael Nielsen's Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Free online book
- [Stanford CS231n](https://cs231n.stanford.edu/) - Deep learning for computer vision
- [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/abs/1802.01528) - Mathematical foundations

---

## üéì Exercises

### Beginner

1. **Train the network** and observe how loss decreases over iterations
2. **Toggle data augmentation** and compare accuracy with/without it
3. **Try different learning rates** and observe the training behavior

### Intermediate

4. **Modify the network architecture** - Add more neurons to hidden layers
5. **Implement a new activation function** (ReLU: `max(0, x)`)
6. **Add a third hidden layer** and compare training

### Advanced

7. **Implement momentum** for faster convergence
8. **Add dropout** for regularization
9. **Implement batch normalization**

---

*Happy Learning! üöÄ*

*This guide was written to accompany the Multi-Layer Perceptron digit recognition project. The concepts here apply to all neural networks!*
