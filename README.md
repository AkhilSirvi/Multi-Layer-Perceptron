# Multi-Layer Perceptron Neural Network

<p align="center">
  <img src="https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black" alt="JavaScript">
  <img src="https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white" alt="HTML5">
  <img src="https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white" alt="CSS3">
  <img src="https://img.shields.io/badge/Chart.js-FF6384?style=for-the-badge&logo=chartdotjs&logoColor=white" alt="Chart.js">
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Neural_Network-MLP-6366f1?style=flat-square" alt="Neural Network">
  <img src="https://img.shields.io/badge/Layers-3-10b981?style=flat-square" alt="Layers">
  <img src="https://img.shields.io/badge/Neurons-400â†’16â†’16â†’10-f59e0b?style=flat-square" alt="Neurons">
  <img src="https://img.shields.io/badge/Activation-TanH_|_Softmax-ec4899?style=flat-square" alt="Activation">
</p>

<p align="center">
  <img src="https://img.shields.io/github/license/AkhilSirvi/Multi-Layer-Perceptron?style=flat-square&color=22c55e" alt="License">
  <img src="https://img.shields.io/github/stars/AkhilSirvi/Multi-Layer-Perceptron?style=flat-square&color=eab308" alt="Stars">
  <img src="https://img.shields.io/github/forks/AkhilSirvi/Multi-Layer-Perceptron?style=flat-square&color=3b82f6" alt="Forks">
  <img src="https://img.shields.io/badge/PRs-Welcome-22d3ee?style=flat-square" alt="PRs Welcome">
</p>

---

A fully functional neural network system for handwritten digit recognition, built from scratch using vanilla JavaScript. This project implements a 3-layer Multi-Layer Perceptron (MLP) that can recognize digits 0-9 drawn by users.

ğŸ”— **Live Demo:** [https://akhilsirvi.github.io/Multi-Layer-Perceptron/src/index.html](https://akhilsirvi.github.io/Multi-Layer-Perceptron/src/index.html)

<p align="center">
  <img src="https://user-images.githubusercontent.com/133025697/250265474-5206e6e2-7dcc-4a61-9d8b-cbffd7e57da6.png" alt="Neural Network System Screenshot">
</p>

---

## Table of Contents

- [Features](#features)
- [Neural Network Architecture](#neural-network-architecture)
- [How It Works](#how-it-works)
  - [Forward Propagation](#forward-propagation)
  - [Backpropagation](#backpropagation)
  - [Activation Functions](#activation-functions)
- [Data Augmentation](#data-augmentation)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Hyperparameters](#hyperparameters)
- [Technologies Used](#technologies-used)
- [Author](#author)
- [License](#license)

---

## Features

- âœï¸ **Interactive Drawing Canvas** - 20x20 pixel grid for drawing digits
- ğŸ§  **Real-time Prediction** - Instant digit recognition with confidence percentages
- ğŸ“Š **Training Visualization** - Live chart showing cost and accuracy during training
- âš™ï¸ **Adjustable Hyperparameters** - Modify learning rate, regularization, and training iterations
- ğŸ“± **Responsive Design** - Works on both desktop and mobile devices
- ğŸ”„ **Data Augmentation** - Automatic translation, rotation, and noise for better generalization
- ğŸ’¾ **Pre-trained Weights** - Ready to use immediately without training

---

## Neural Network Architecture

The network uses a classic Multi-Layer Perceptron (MLP) architecture with 3 layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           NETWORK ARCHITECTURE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   INPUT LAYER          HIDDEN LAYER 1      HIDDEN LAYER 2      OUTPUT       â”‚
â”‚   (400 neurons)        (16 neurons)        (16 neurons)        (10 neurons) â”‚
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”                 â”Œâ”€â”€â”€â”               â”Œâ”€â”€â”€â”               â”Œâ”€â”€â”€â”       â”‚
â”‚   â”‚ 1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 0 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤                 â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚ 2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤                 â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚ 3 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 3 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 3 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤      W1         â”œâ”€â”€â”€â”¤      W2       â”œâ”€â”€â”€â”¤      W3       â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚...â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚...â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚...â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚...â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤  (6,400 weights)â”œâ”€â”€â”€â”¤ (256 weights) â”œâ”€â”€â”€â”¤ (160 weights) â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚399â”‚                 â”‚15 â”‚               â”‚15 â”‚               â”‚ 8 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤                 â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚400â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚16 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚16 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 9 â”‚       â”‚
â”‚   â””â”€â”€â”€â”˜                 â””â”€â”€â”€â”˜               â””â”€â”€â”€â”˜               â””â”€â”€â”€â”˜       â”‚
â”‚                                                                              â”‚
â”‚   20Ã—20 pixel grid      TanH activation     TanH activation     Softmax     â”‚
â”‚   (flattened)           + Bias              + Bias              (probabilities)â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Details

| Layer | Neurons | Weights | Biases | Activation |
|-------|---------|---------|--------|------------|
| Input (Aâ‚€) | 400 | - | - | - |
| Hidden 1 (Aâ‚) | 16 | 6,400 | 16 | TanH |
| Hidden 2 (Aâ‚‚) | 16 | 256 | 16 | TanH |
| Output (Aâ‚ƒ) | 10 | 160 | 10 | Softmax |

**Total Parameters:** 6,858 (6,816 weights + 42 biases)

---

## How It Works

### Forward Propagation

Forward propagation passes input data through the network to produce predictions:

```
For each layer l:
    Z[l] = W[l] Â· A[l-1] + B[l]    (weighted sum + bias)
    A[l] = g(Z[l])                  (apply activation function)
```

**Step-by-step process:**
1. **Input Layer (Aâ‚€):** Flatten the 20Ã—20 drawing grid into a 400-element array (0 = white, 1 = black)
2. **Hidden Layer 1 (Aâ‚):** Compute `Aâ‚ = TanH(Wâ‚ Â· Aâ‚€ + Bâ‚)`
3. **Hidden Layer 2 (Aâ‚‚):** Compute `Aâ‚‚ = TanH(Wâ‚‚ Â· Aâ‚ + Bâ‚‚)`
4. **Output Layer (Aâ‚ƒ):** Compute `Aâ‚ƒ = Softmax(Wâ‚ƒ Â· Aâ‚‚ + Bâ‚ƒ)`

The output is a probability distribution over digits 0-9. The digit with the highest probability is the prediction.

### Backpropagation

Backpropagation computes gradients to update weights and biases, minimizing the cost function:

```
Cost Function: Cross-Entropy Loss
J = -(1/m) Ã— Î£ log(p_correct)

where:
- m = number of training examples
- p_correct = predicted probability of the true class
```

**Gradient Computation (backward pass):**

```
Output Layer:
    dZâ‚ƒ = Aâ‚ƒ - Y                    (softmax gradient with cross-entropy)
    dWâ‚ƒ = (1/m) Ã— dZâ‚ƒáµ€ Â· Aâ‚‚
    dBâ‚ƒ = (1/m) Ã— Î£ dZâ‚ƒ

Hidden Layer 2:
    dZâ‚‚ = (Wâ‚ƒáµ€ Â· dZâ‚ƒ) âŠ™ g'(Aâ‚‚)     (âŠ™ = element-wise multiplication)
    dWâ‚‚ = (1/m) Ã— dZâ‚‚áµ€ Â· Aâ‚
    dBâ‚‚ = (1/m) Ã— Î£ dZâ‚‚

Hidden Layer 1:
    dZâ‚ = (Wâ‚‚áµ€ Â· dZâ‚‚) âŠ™ g'(Aâ‚)
    dWâ‚ = (1/m) Ã— dZâ‚áµ€ Â· Aâ‚€
    dBâ‚ = (1/m) Ã— Î£ dZâ‚
```

**Parameter Update (Gradient Descent with L2 Regularization):**

```
W = W - Î± Ã— dW + Î» Ã— W
B = B - Î± Ã— dB

where:
- Î± = learning rate (controls step size)
- Î» = regularization coefficient (prevents overfitting)
```

### Activation Functions

#### TanH (Hyperbolic Tangent)
Used in hidden layers to introduce non-linearity:

```
tanh(z) = (eá¶» - eâ»á¶») / (eá¶» + eâ»á¶»)

Output range: (-1, 1)
Derivative: g'(a) = 1 - aÂ²
```

**Properties:**
- Zero-centered output (helps with convergence)
- Stronger gradients than sigmoid
- Saturates for very large/small inputs

#### Softmax
Used in the output layer for multi-class classification:

```
softmax(záµ¢) = eá¶»â± / Î£â±¼ eá¶»Ê²

Output: Probability distribution (all values sum to 1)
```

**Properties:**
- Converts raw scores to probabilities
- Each output represents P(digit = i | input)
- Highest probability = predicted digit

---

## Data Augmentation

To improve generalization and prevent overfitting, training data is augmented with random transformations:

### 1. Translation (Position Shift)
```
Shifts the image randomly by -4 to +4 pixels in both X and Y directions
Helps the model recognize digits regardless of position
```

### 2. Rotation
```
Rotates the image randomly by -15Â° to +15Â°
Helps the model recognize slightly tilted digits
Uses 2D rotation matrix:
    x' = xÂ·cos(Î¸) - yÂ·sin(Î¸)
    y' = xÂ·sin(Î¸) + yÂ·cos(Î¸)
```

### 3. Noise Addition
```
Randomly flips white pixels (0) to black (1) with 1% probability
Adds robustness against noisy input
```

---

## Project Structure

```
Multi-Layer-Perceptron/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.html          # Main HTML page with UI elements
â”‚   â””â”€â”€ style.css           # Stylesheet for responsive design
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ script.js           # Main neural network logic
â”‚   â”‚                         - Pixel grid creation
â”‚   â”‚                         - Event handlers (mouse/touch)
â”‚   â”‚                         - Training loop (backpropagation)
â”‚   â”‚                         - Prediction (forward propagation)
â”‚   â”‚                         - Data augmentation functions
â”‚   â”œâ”€â”€ maths.js            # Mathematical operations
â”‚   â”‚                         - Activation functions (TanH, Softmax)
â”‚   â”‚                         - Matrix operations
â”‚   â”‚                         - Gradient descent updates
â”‚   â”œâ”€â”€ graph.js            # Training visualization
â”‚   â”‚                         - Chart.js integration
â”‚   â”‚                         - Cost/accuracy plotting
â”‚   â””â”€â”€ data.js             # Pre-trained weights & training data
â”‚                             - Weight matrices (W1, W2, W3)
â”‚                             - Bias vectors (B1, B2, B3)
â”‚                             - MNIST-style training examples
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ LICENSE                 # MIT License
â””â”€â”€ SECURITY.md             # Security policy
```

---

## Getting Started

### Prerequisites
- A modern web browser (Chrome, Firefox, Safari, Edge)
- No additional dependencies required!

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/akhilsirvi/Multi-Layer-Perceptron.git
   ```

2. **Navigate to the project:**
   ```bash
   cd Multi-Layer-Perceptron
   ```

3. **Open in browser:**
   - Open `src/index.html` in your web browser
   - Or use a local server:
     ```bash
     # Using Python
     python -m http.server 8000
     
     # Using Node.js
     npx serve
     ```

---

## Usage

### Drawing and Recognition

1. **Draw a digit** on the 20Ã—20 pixel grid using your mouse or touch
2. **Click "Enter your written data"** to get the prediction
3. The output panel shows:
   - The predicted digit (highest probability)
   - Confidence percentages for all digits (0-9)

### Training the Network

1. **Adjust hyperparameters** (optional):
   - Alpha (Learning Rate): Default 0.1
   - Training Length: Default 20 iterations
   - Lambda (Regularization): Default 0.000001

2. **Click "Train the neural network"**
3. Watch the progress bar and metrics update
4. The chart shows cost decreasing and accuracy increasing

---

## Hyperparameters

| Parameter | Default | Description | Recommended Range |
|-----------|---------|-------------|-------------------|
| **Alpha (Î±)** | 0.1 | Learning rate - controls gradient descent step size | 0.001 - 1.0 |
| **Training Length** | 20 | Number of training iterations/epochs | 10 - 100 |
| **Lambda (Î»)** | 0.000001 | L2 regularization coefficient | 0.000001 - 0.01 |

### Tuning Tips

- **High cost, low accuracy?** â†’ Increase training length or learning rate
- **Overfitting (good training, poor testing)?** â†’ Increase lambda
- **Training unstable?** â†’ Decrease learning rate
- **Training too slow?** â†’ Increase learning rate (carefully)

---

## Technologies Used

- **HTML5** - Structure and semantic markup
- **CSS3** - Responsive styling with Flexbox and Grid
- **JavaScript (ES6+)** - Core neural network implementation
- **Chart.js** - Training metrics visualization

### No External ML Libraries!
This project implements neural networks from scratch without TensorFlow, PyTorch, or any ML frameworks. All matrix operations, activation functions, and backpropagation are coded manually for educational purposes.

---

## Mathematical Formulas Reference

### Forward Propagation
```
A[l] = g(W[l] Â· A[l-1] + B[l])
```

### Cost Function (Cross-Entropy)
```
J = -(1/m) Ã— Î£áµ¢ Î£â±¼ yáµ¢â±¼ Ã— log(Ã¢áµ¢â±¼)
```

### Backpropagation Gradients
```
dW[l] = (1/m) Ã— dZ[l] Â· A[l-1]áµ€
dB[l] = (1/m) Ã— Î£ dZ[l]
dZ[l-1] = W[l]áµ€ Â· dZ[l] âŠ™ g'(Z[l-1])
```

### Gradient Descent Update
```
W[l] := W[l] - Î± Ã— dW[l] + Î» Ã— W[l]
B[l] := B[l] - Î± Ã— dB[l]
```

---

## Author

**Akhil Sirvi**

- GitHub: [@akhilsirvi](https://github.com/akhilsirvi)

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- Inspired by the MNIST handwritten digit dataset
- Thanks to the deep learning community for educational resources
- Chart.js for the excellent charting library

---

<p align="center">
  Made with â¤ï¸ and JavaScript
</p>
