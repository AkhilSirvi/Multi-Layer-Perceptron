# Multi-Layer Perceptron Neural Network

<p align="center">
  <img src="https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black" alt="JavaScript">
  <img src="https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white" alt="HTML5">
  <img src="https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white" alt="CSS3">
  <img src="https://img.shields.io/badge/Chart.js-FF6384?style=for-the-badge&logo=chartdotjs&logoColor=white" alt="Chart.js">
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Neural_Network-MLP-6366f1?style=flat-square" alt="Neural Network">
  <img src="https://img.shields.io/badge/Layers-3-10b981?style=flat-square" alt="Layers">
  <img src="https://img.shields.io/badge/Neurons-784â†’128â†’64â†’10-f59e0b?style=flat-square" alt="Neurons">
  <img src="https://img.shields.io/badge/Activation-TanH_|_Leaky ReLU_|_ReLU_|_Softmax-ec4899?style=flat-square" alt="Activation">
</p>

<p align="center">
  <img src="https://img.shields.io/github/license/AkhilSirvi/Multi-Layer-Perceptron?style=flat-square&color=22c55e" alt="License">
  <img src="https://img.shields.io/github/stars/AkhilSirvi/Multi-Layer-Perceptron?style=flat-square&color=eab308" alt="Stars">
  <img src="https://img.shields.io/github/forks/AkhilSirvi/Multi-Layer-Perceptron?style=flat-square&color=3b82f6" alt="Forks">
  <img src="https://img.shields.io/badge/PRs-Welcome-22d3ee?style=flat-square" alt="PRs Welcome">
</p>

---

A fully functional neural network system for handwritten digit recognition, built from scratch using vanilla JavaScript. This project implements a 3-layer Multi-Layer Perceptron (MLP) that can recognize digits 0-9 drawn by users.

ğŸ”— **Live Demo:** [https://akhilsirvi.github.io/Multi-Layer-Perceptron/src/index.html](https://akhilsirvi.github.io/Multi-Layer-Perceptron/src/index.html)

<p align="center">
  <img width="1119" height="578" alt="image" src="https://github.com/user-attachments/assets/abd61d8f-80aa-41c6-b1d5-8b85ad8890ea" />
</p>

---

## Table of Contents

- [Features](#features)
- [Neural Network Architecture](#neural-network-architecture)
- [How It Works](#how-it-works)
  - [Forward Propagation](#forward-propagation)
  - [Backpropagation](#backpropagation)
  - [Activation Functions](#activation-functions)
- [Data Augmentation](#data-augmentation)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Hyperparameters](#hyperparameters)
- [Technologies Used](#technologies-used)
- [Author](#author)
- [License](#license)

---

## Features

- **Interactive Drawing Canvas** - 28x28 pixel grid for drawing digits
- **Real-time Prediction** - Instant digit recognition with confidence percentages
- **Training Visualization** - Live chart showing cost and accuracy during training
- **Adjustable Hyperparameters** - Modify learning rate, regularization, and training iterations
- **Responsive Design** - Works on both desktop and mobile devices
- **Data Augmentation** - Automatic translation, rotation, and noise for better generalization
- **Pre-trained Weights** - Ready to use immediately without training

---

## Neural Network Architecture

The network uses a classic Multi-Layer Perceptron (MLP) architecture with 3 layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           NETWORK ARCHITECTURE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   INPUT LAYER          HIDDEN LAYER 1      HIDDEN LAYER 2      OUTPUT       â”‚
â”‚   (784 neurons)        (128 neurons)       (64 neurons)        (10 neurons) â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”                 â”Œâ”€â”€â”€â”               â”Œâ”€â”€â”€â”               â”Œâ”€â”€â”€â”       â”‚
â”‚   â”‚ 1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 0 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤                 â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚ 2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤                 â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚ 3 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 3 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 3 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤      W1         â”œâ”€â”€â”€â”¤      W2       â”œâ”€â”€â”€â”¤      W3       â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚...â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚...â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚...â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚...â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤(100,352 weights)â”œâ”€â”€â”€â”¤(8192 weights) â”œâ”€â”€â”€â”¤(640 weights)  â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚783â”‚                 â”‚127â”‚               â”‚63 â”‚               â”‚ 8 â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”¤                 â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤               â”œâ”€â”€â”€â”¤       â”‚
â”‚   â”‚784â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚128â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚64 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 9 â”‚       â”‚
â”‚   â””â”€â”€â”€â”˜                 â””â”€â”€â”€â”˜               â””â”€â”€â”€â”˜               â””â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚   28Ã—28 pixel grid      Leaky ReLU         Leaky ReLU           Softmax     â”‚
â”‚   (flattened)           + Bias              + Bias           (probabilities)â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Details

| Layer | Neurons | Weights | Biases | Activation |
|-------|---------|---------|--------|------------|
| Input (Aâ‚€) | 784 | - | - | - |
| Hidden 1 (Aâ‚) | 128 | 100,352 | 128 | Leaky ReLU |
| Hidden 2 (Aâ‚‚) | 64 | 8192 | 64 | Leaky ReLU |
| Output (Aâ‚ƒ) | 10 | 640 | 10 | Softmax |

**Total Parameters:** 109,386 (109,184 weights + 202 biases)

---

## How It Works

### Forward Propagation

Forward propagation passes input data through the network to produce predictions:

### Layer Computation

For each layer $l$:

$$
Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}
$$

$$
A^{[l]} = g\left(Z^{[l]}\right)
$$

**Step-by-step process:**
1. **Input Layer (Aâ‚€):** Flatten the 28Ã—28 drawing grid into a 784-element array (0 = white, 1 = black)
2. **Hidden Layer 1 (Aâ‚):** Compute `Aâ‚ = TanH(Wâ‚ Â· Aâ‚€ + Bâ‚)`
3. **Hidden Layer 2 (Aâ‚‚):** Compute `Aâ‚‚ = TanH(Wâ‚‚ Â· Aâ‚ + Bâ‚‚)`
4. **Output Layer (Aâ‚ƒ):** Compute `Aâ‚ƒ = Softmax(Wâ‚ƒ Â· Aâ‚‚ + Bâ‚ƒ)`

The output is a probability distribution over digits 0-9. The digit with the highest probability is the prediction.

### Backpropagation

Backpropagation computes gradients to update weights and biases, minimizing the cost function:

### Cost Function â€” Cross-Entropy Loss

$$
J = -\frac{1}{m} \sum_{i=1}^{m} \log\left(p_{\text{correct}}^{(i)}\right)
$$

where  

- $m$ = number of training examples  
- $p_{\text{correct}}^{(i)}$ = predicted probability of the true class for example $i$

**Gradient Computation (backward pass):**

### Backpropagation

#### Output Layer

$$
dZ^{[3]} = A^{[3]} - Y
$$

$$
dW^{[3]} = \frac{1}{m} \, dZ^{[3]} (A^{[2]})^T
$$

$$
db^{[3]} = \frac{1}{m} \sum dZ^{[3]}
$$

---

#### Hidden Layer 2

$$
dZ^{[2]} = (W^{[3]})^T dZ^{[3]} \odot g'(A^{[2]})
$$

$$
dW^{[2]} = \frac{1}{m} \, dZ^{[2]} (A^{[1]})^T
$$

$$
db^{[2]} = \frac{1}{m} \sum dZ^{[2]}
$$

---

#### Hidden Layer 1

$$
dZ^{[1]} = (W^{[2]})^T dZ^{[2]} \odot g'(A^{[1]})
$$

$$
dW^{[1]} = \frac{1}{m} \, dZ^{[1]} (A^{[0]})^T
$$

$$
db^{[1]} = \frac{1}{m} \sum dZ^{[1]}
$$

**Parameter Update (Gradient Descent with L2 Regularization):**

### Parameter Update Rule

$$
W = W - \alpha\, dW - \lambda\, W
$$

$$
B = B - \alpha\, dB
$$

where  

- $\alpha$ = learning rate (controls step size)  
- $\lambda$ = regularization coefficient (prevents overfitting)

### Activation Functions

#### TanH (Hyperbolic Tangent)
Used in hidden layers to introduce non-linearity:

### Tanh Activation

$$
\tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
$$

Output range: $(-1, 1)$

Derivative:

$$
g'(z) = 1 - \tanh^2(z)
$$

**Properties:**
- Zero-centered output (helps with convergence)
- Stronger gradients than sigmoid
- Saturates for very large/small inputs

#### Softmax
Used in the output layer for multi-class classification:

### Softmax Function

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

Output: Probability distribution (all values sum to 1)

**Properties:**
- Converts raw scores to probabilities
- Each output represents P(digit = i | input)
- Highest probability = predicted digit

---

## Data Augmentation

To improve generalization and prevent overfitting, training data is augmented with random transformations:

### 1. Translation (Position Shift)
```
Shifts the image randomly by -2 to +2 pixels in both X and Y directions
Helps the model recognize digits regardless of position
```

### 2. Rotation
```
Rotates the image randomly by -5Â° to +5Â°
Helps the model recognize slightly tilted digits
Uses 2D rotation matrix:
    x' = xÂ·cos(Î¸) - yÂ·sin(Î¸)
    y' = xÂ·sin(Î¸) + yÂ·cos(Î¸)
```

### 3. Noise Addition
```
Randomly flips pixels random gaussian noise (3% std dev)
Adds robustness against noisy input
```

---

## Project Structure

```
Multi-Layer-Perceptron/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.html          # Main HTML page with UI elements
â”‚   â””â”€â”€ style.css           # Stylesheet for responsive design
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ script.js           # Main neural network logic
â”‚   â”‚                         - Pixel grid creation
â”‚   â”‚                         - Event handlers (mouse/touch)
â”‚   â”‚                         - Training loop (backpropagation)
â”‚   â”‚                         - Prediction (forward propagation)
â”‚   â”‚                         - Data augmentation functions
â”‚   â”œâ”€â”€ maths.js            # Mathematical operations
â”‚   â”‚                         - Activation functions
â”‚   â”‚                         - Matrix operations
â”‚   â”‚                         - Gradient descent updates
â”‚   â”œâ”€â”€ graph.js            # Training visualization
â”‚   â”‚                         - Chart.js integration
â”‚   â”‚                         - Cost/accuracy plotting
â”‚   â””â”€â”€ data.js             # Pre-trained weights & training data
â”‚                             - Weight matrices (W1, W2, W3)
â”‚                             - Bias vectors (B1, B2, B3)
â”‚                             - MNIST-style training examples
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ LICENSE                 # MIT License
â””â”€â”€ SECURITY.md             # Security policy
```

---

## Getting Started

### Prerequisites
- A modern web browser (Chrome, Firefox, Safari, Edge)
- No additional dependencies required

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/akhilsirvi/Multi-Layer-Perceptron.git
   ```

2. **Navigate to the project:**
   ```bash
   cd Multi-Layer-Perceptron
   ```

3. **Open in browser:**
   - Open `src/index.html` in your web browser
   - Or use a local server:
     ```bash
     # Using Python
     python -m http.server 8000
     
     # Using Node.js
     npx serve
     ```

---

## Usage

### Drawing and Recognition

1. **Draw a digit** on the 28Ã—28 pixel grid using your mouse or touch
2. **Click "Enter your written data"** to get the prediction
3. The output panel shows:
   - The predicted digit (highest probability)
   - Confidence percentages for all digits (0-9)

### Training the Network

1. **Adjust hyperparameters** (optional):
   - Alpha (Learning Rate): Default 0.1
   - Training Length: Default 20 iterations
   - Lambda (Regularization): Default 0.000001

2. **Click "Train the neural network"**
3. Watch the progress bar and metrics update
4. The chart shows cost decreasing and accuracy increasing

---

## Hyperparameters

| Parameter | Default | Description | Recommended Range |
|-----------|---------|-------------|-------------------|
| **Alpha (Î±)** | 0.1 | Learning rate - controls gradient descent step size | 0.001 - 1.0 |
| **Training Length** | 20 | Number of training iterations/epochs | 10 - 100 |
| **Lambda (Î»)** | 0.000001 | L2 regularization coefficient | 0.000001 - 0.01 |

### Tuning Tips

- **High cost, low accuracy?** â†’ Increase training length or learning rate
- **Overfitting (good training, poor testing)?** â†’ Increase lambda
- **Training unstable?** â†’ Decrease learning rate
- **Training too slow?** â†’ Increase learning rate (carefully)

---

## Technologies Used

- **HTML5** - Structure and semantic markup
- **CSS3** - Responsive styling with Flexbox and Grid
- **JavaScript (ES6+)** - Core neural network implementation
- **Chart.js** - Training metrics visualization

### No External ML Libraries!
This project implements neural networks from scratch without TensorFlow, PyTorch, or any ML frameworks. All matrix operations, activation functions, and backpropagation are coded manually for educational purposes.

## Mathematical Formulas Reference

### Forward Propagation

$$
A^{[l]} = g\left(W^{[l]} A^{[l-1]} + b^{[l]}\right)
$$

### Cost Function (Cross-Entropy)

$$
J = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{n} y_{ij} \log(\hat{a}_{ij})
$$

### Backpropagation Gradients

$$
dW^{[l]} = \frac{1}{m} dZ^{[l]} (A^{[l-1]})^T
$$

$$
db^{[l]} = \frac{1}{m} \sum dZ^{[l]}
$$

$$
dZ^{[l-1]} = (W^{[l]})^T dZ^{[l]} \odot g'(Z^{[l-1]})
$$

### Gradient Descent Update

$$
W^{[l]} := W^{[l]} - \alpha dW^{[l]} - \lambda W^{[l]}
$$

$$
b^{[l]} := b^{[l]} - \alpha db^{[l]}
$$

---

## Author

**Akhil Sirvi**

- GitHub: [@akhilsirvi](https://github.com/akhilsirvi)

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- Inspired by the MNIST handwritten digit dataset
- Thanks to the deep learning community for educational resources
- Chart.js for the excellent charting library
